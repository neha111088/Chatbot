{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4f68d58",
   "metadata": {},
   "source": [
    "# CHATBOT FOR DATA SCIENCE QUERIES\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1dd542",
   "metadata": {},
   "source": [
    "### IMPORTING LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96a1997f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "import random\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c88844c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc246d3",
   "metadata": {},
   "source": [
    "### TEXT PRE-PROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e4a7ca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\urade\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\urade\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "f=open(\"intents_1.json\", \"r\", errors = \"ignore\")\n",
    "corpus = f.read()\n",
    "corpus = corpus.lower() #converts text into lower case\n",
    "nltk.download(\"punkt\") #using punkt tokenizer\n",
    "nltk.download(\"wordnet\") #using wordnet dictionary\n",
    "sent_tokens = nltk.sent_tokenize(corpus) #converts corpus into list of sentences\n",
    "word_tokens = nltk.word_tokenize(corpus) #converts corpus into list of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88b6e808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\"data_science\"\n",
      "data science is a blend of various tools, algorithms, and machine learning principles with the goal to discover hidden patterns from the raw data.\n",
      "\"confusion matrix\"\n",
      "the confusion matrix is a 2x2 table that contains 4 outputs provided by the binary classifier. various measures, such as error-rate, accuracy, specificity, sensitivity, precision and recall are derived from it.\n",
      "\"supervised_learning\",\n",
      "supervised learning is the machine learning task of inferring a function from labeled training data. the training data consist of a set of training examples of algorithms such as support vector machines, regression, naive bayes, decision trees, k-nearest neighbor algorithm and neural networks\n",
      "\"unsupervised_learning\"\n",
      "unsupervised learning is a type of machine learning algorithm used to draw inferences from datasets consisting of input data without labelled responses.algorithms: clustering, anomaly detection, neural networks and latent variable models\"]\n",
      "\"machine_learning\",\n",
      "machine learning explores the study and construction of algorithms that can learn from and make predictions on data.it is closely related to computational statistics used to devise complex models and algorithms that lend themselves to a prediction which in commercial use is known as predictive analytics.\n",
      "\"nlp\"\n",
      "natural language processing, or nlp for short, is broadly defined as the automatic manipulation of natural language, like speech and text, by software.\n",
      "\"tfidf\"\n",
      "tfidf is short for term frequency-inverse document frequency, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus. it is often used as a weighting factor in information retrieval and text mining.\n",
      "\"random_forest\",\n",
      "random forest is a versatile machine learning method capable of performing both regression and classification tasks. it is also used for dimensionality reduction, treats missing values, outlier values. it is a type of ensemble learning method, where a group of weak models combine to form a powerful model.\n",
      "\"deep_learning\",\n",
      "deep learning is nothing but a paradigm of machine learning which has shown incredible promise in recent years. this is because of the fact that deep learning shows a great analogy with the functioning of the human brain.\n",
      "\"neural_netwrok\",\n",
      "a neural network in data science aims to imitate a human brain neuron, where different neurons combine together and perform a task. it learns the generalizations or patterns from data and uses this knowledge to predict output for new data, without any human intervention.\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0071e9f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n\"data_science\"\\ndata science is a blend of various tools, algorithms, and machine learning principles with the goal to discover hidden patterns from the raw data.',\n",
       " '\"confusion matrix\"\\nthe confusion matrix is a 2x2 table that contains 4 outputs provided by the binary classifier.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokens[:2] #prints first two sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "546d4db9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"''\", 'data_science']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokens[:2] #prints first two words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f21068a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#wordnet is a semantically oriented dictionary of english included in NLTK\n",
    "lemmer = nltk.stem.WordNetLemmatizer()\n",
    "def LemTokens(tokens):\n",
    "    return [lemmer.lemmatize(token) for token in tokens]\n",
    "remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
    "def LemNormalize(text):\n",
    "    return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69b5c38",
   "metadata": {},
   "source": [
    "### BOT GREETINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c573af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "greet_inputs = (\"hello\",\"hey\",\"wassup\")\n",
    "greet_response = (\"Hey there!\",\"How can I help you?\",\"Glad to talk to you!\" )\n",
    "\n",
    "def greet(sentence):\n",
    "    for word in sentence.split():\n",
    "        if word.lower() in greet_inputs:\n",
    "            return random.choice(greet_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563fc6f8",
   "metadata": {},
   "source": [
    "###  BOT RESPONSE GENERATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6cb0e064",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "59bab02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def response(user_response):\n",
    "    robo1_response=\"\"\n",
    "    TfidfVec = TfidfVectorizer(tokenizer= LemNormalize, stop_words = \"english\")\n",
    "    tfidf = TfidfVec.fit_transform(sent_tokens)\n",
    "    vals = cosine_similarity(tfidf[-1], tfidf)\n",
    "    idx = vals.argsort()[0][-2]\n",
    "    \n",
    "    flat = vals.flatten()\n",
    "    flat.sort()\n",
    "    \n",
    "    req_tfidf=flat[-2]\n",
    "    \n",
    "    if(req_tfidf==0):\n",
    "        robo1_response = robo1_response+\"im sorry! i couldnt get you\"\n",
    "        return robo1_response\n",
    "    else:\n",
    "        robo1_response = robo1_response+sent_tokens[idx]\n",
    "        return robo1_response\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e31354",
   "metadata": {},
   "source": [
    "### DEFINING CONVERSATION AND START TO END PROTOCOLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ac8687",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I-cup:HOLA!My name is I-cup and I am your assistant BOT.Lets have a conversation!Type bye in case you wish to leave!\n"
     ]
    }
   ],
   "source": [
    "flag = True\n",
    "print(\"I-cup:HOLA!My name is I-cup and I am your assistant BOT.Lets have a conversation!Type bye in case you wish to leave!\")\n",
    "while(flag==True):\n",
    "    user_response = input()\n",
    "    user_response = user_response.lower()\n",
    "    if(user_response != \"bye\"):\n",
    "        if(user_response == \"thanks\" or user_response ==\"thank you\"):\n",
    "            flag = False\n",
    "            print(\"I-cup: you are welcome...\")\n",
    "        else:\n",
    "            if(greet(user_response)!= None):\n",
    "                print(\"I-cup: \" + greet(user_response))\n",
    "            else:\n",
    "                sent_tokens.append(user_response)\n",
    "                word_tokens=word_tokens+nltk.word_tokenize(user_response)\n",
    "                final_words = list(set(word_tokens))\n",
    "                print(\"I-cup: \",end=\"\")\n",
    "                print(response(user_response))\n",
    "                sent_tokens.remove(user_response)\n",
    "    else:\n",
    "        flag=False\n",
    "        print(\"I-cup: goodbye! take care <3\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
